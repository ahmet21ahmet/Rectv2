+ 25
- 243
DeÄŸiÅŸtirilen satÄ±rlar: 25 ekleme ve 243 silme
Orijinal dosya satÄ±r numarasÄ±	FarklÄ± satÄ±r numarasÄ±	Fark satÄ±rÄ± deÄŸiÅŸikliÄŸi
@@ -1,247 +1,29 @@
# dizipal_episodes_selenium_m3u.py
# Gereksinimler:
# pip install selenium webdriver-manager beautifulsoup4
import time
import json
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, WebDriverException
# -------------------------
# Ayarlar
# -------------------------
DOMAIN_URL = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizipal.txt"
JSON_FILE = "dizipal/diziler.json"
OUTPUT_FILE = "dizipalyerlidizi.m3u"
# TarayÄ±cÄ± ve bekleme ayarlarÄ±
INITIAL_PAGE_WAIT = 30       # Ana sayfa iÃ§in sabit bekleme (isteÄŸin Ã¼zerine)
PAGE_LOAD_MAX_WAIT = 30      # Her sayfa iÃ§in maksimum bekleme (saniye)
HTML_PRINT_LIMIT = 2000      # Terminale yazdÄ±rÄ±lacak HTML uzunluÄŸu (karakter)
RETRY_COUNT = 1              # EÄŸer istersen sayfa aÃ§mada retry koyabilirsin
# -------------------------
# Selenium baÅŸlatma (Chrome)
# -------------------------
def make_chrome_driver(headless=True):
    options = webdriver.ChromeOptions()
    # Headless kullanmak istersen True bÄ±rak, bazÄ± siteler headless'i algÄ±lar; gerekirse False yap
    if headless:
        options.add_argument("--headless=new")  # chromium 109+ iÃ§in yeni headless daha stabil
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    # TarayÄ±cÄ± fingerprint azaltma
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    # Servisi oluÅŸtur (webdriver-manager ile)
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    # Navigator.webdriver Ã¶zelliÄŸini gizleme (CDP injection)
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                window.__selenium_forced_navigator_webdriver = undefined;
                """
            },
        )
    except Exception:
        pass
    return driver
# -------------------------
# Sayfa yÃ¼kleme / bekleme
# -------------------------
def load_page_and_wait(driver, url, max_wait=PAGE_LOAD_MAX_WAIT, check_strings=None):
    """
    driver.get(url) yapar, sonra max_wait sÃ¼resince sayfa kaynaÄŸÄ±nÄ± kontrol eder.
    check_strings: eÄŸer verilen listede bir ifade sayfada bulunuyorsa 'baÅŸarÄ±lÄ±' sayÄ±lÄ±r.
    EÄŸer check_strings None ise, "Just a moment" baÅŸlÄ±ÄŸÄ± kalkana kadar bekler.
    DÃ¶nen: (page_source, status_ok_bool)
    """
    driver.get(url)
    # Windows / env koÅŸullarÄ±nda ilk render iÃ§in kÃ¼Ã§Ã¼k bekleme
    start = time.time()
    while True:
        page_source = driver.page_source
        title = driver.title or ""
        elapsed = time.time() - start
        # 1) EÄŸer sayfa title "Just a moment" iÃ§eriyorsa muhtemelen cloudflare challenge
        if check_strings:
            # eÄŸer herhangi bir check_string bulunuyorsa baÅŸarÄ±lÄ± kabul et
            for s in check_strings:
                if s.lower() in page_source.lower():
                    return page_source, True
        else:
            # default kontrol: title "Just a moment" deÄŸilse veya "Enable JavaScript" yoksa baÅŸarÄ±lÄ±
            if "just a moment" not in title.lower() and "enable javascript" not in page_source.lower():
                # ayrÄ±ca sayfada episode-item gibi beklenen seÃ§ici varsa da baÅŸarÄ±lÄ±
                if "episode-item" in page_source or re.search(r"IMDB\s*Puan", page_source, re.I):
                    return page_source, True
                # eÄŸer challenge yazÄ±sÄ± yok ama iÃ§erik deÄŸiÅŸmiÅŸse de kabul et
                if "checking your browser" not in page_source.lower():
                    return page_source, True
        if elapsed >= max_wait:
            return page_source, False
        time.sleep(1)
# -------------------------
# Scrape fonksiyonu (BeautifulSoup ile pars)
# -------------------------
def scrape_series_episodes_from_html(html_text):
    soup = BeautifulSoup(html_text, "html.parser")
    imdb_div = soup.find("div", class_="key", string=re.compile(r"IMDB", re.I))
    if imdb_div:
        value_div = imdb_div.find_next_sibling("div", class_="value")
        imdb_score = value_div.get_text(strip=True) if value_div else "-"
    else:
        imdb_score = "-"
    episodes = []
    for ep_div in soup.select("div.episode-item a[href]"):
        ep_href = ep_div.get("href")
        ep_title_div = ep_div.select_one("div.episode")
        ep_title = ep_title_div.get_text(strip=True) if ep_title_div else "-"
        episodes.append({"href": ep_href, "title": ep_title})
    # fallback: linkleri tarayÄ±p 'bolum' gibi anahtar kelimeye gÃ¶re al
    if not episodes:
        for a in soup.select("a[href]"):
            href = a.get("href")
            txt = a.get_text(strip=True) or ""
            if href and ("/bolum" in href.lower() or "bÃ¶lÃ¼m" in txt.lower()):
                episodes.append({"href": href, "title": txt or href})
    return {"imdb": imdb_score, "episodes": episodes}
# -------------------------
# Ana akÄ±ÅŸ
# -------------------------
def main():
    # 0) Chrome driver oluÅŸtur
    # EÄŸer Cloudflare Ã§ok sÄ±kÄ±ysa headless=False dene (gÃ¶rÃ¼nÃ¼r tarayÄ±cÄ±).
    headless = True  # gerekirse False yap
    driver = None
    try:
        driver = make_chrome_driver(headless=headless)
    except WebDriverException as e:
        print("âŒ Chrome driver baÅŸlatÄ±lamadÄ±:", e)
        print("Chrome yÃ¼klÃ¼ mÃ¼? Driver/Chrome sÃ¼rÃ¼m uyuÅŸmazlÄ±ÄŸÄ± olabilir.")
        return

    try:
        # 1) DOMAIN_URL'den BASE_URL al
        print("ğŸŒ Domain bilgisi alÄ±nÄ±yor (tarayÄ±cÄ± ile)...")
        # Gitmek iÃ§in direkt requests deÄŸil selenium kullanÄ±yoruz (bazÄ± ortamlar iÃ§in GitHub raw sayfasÄ± aÃ§Ä±lmayabilir; burayÄ± basit bir get olarak da bÄ±rakabilirsin)
        driver.get(DOMAIN_URL)
        time.sleep(1)
        domain_page = driver.page_source
        # raw iÃ§eriÄŸi body iÃ§inde dÃ¼z metin olarak gelecektir; basit regex ile al
        m = re.search(r"https?://[^\s'\"<>]+", domain_page)
        if m:
            BASE_URL = m.group(0).strip()
            print(f"ğŸŒ KullanÄ±lan BASE_URL (bulundu): {BASE_URL}")
        else:
            # fallback: sayfayÄ± direkt text olarak almayÄ± dene
            text_only = re.sub(r"<[^>]+>", "", domain_page)
            BASE_URL = text_only.strip().splitlines()[0].strip()
            print(f"ğŸŒ KullanÄ±lan BASE_URL (fallback): {BASE_URL}")

        # 2) Ana sayfa iÃ§in sabit bekleme
        print(f"â³ Ana sayfa yÃ¼kleniyor, {INITIAL_PAGE_WAIT} saniye bekleniyor...", flush=True)
        time.sleep(INITIAL_PAGE_WAIT)

        # 3) JSON oku
        with open(JSON_FILE, "r", encoding="utf-8") as f:
            series_data = json.load(f)

        # 4) M3U dosyasÄ± aÃ§
        with open(OUTPUT_FILE, "w", encoding="utf-8") as m3u_file:
            print("#EXTM3U", flush=True, file=m3u_file)

            for series_href, info in series_data.items():
                group = info.get("group", "UNKNOWN")
                tvg_logo = info.get("tvg-logo", "")
                print(f"\nğŸ¬ {group} bÃ¶lÃ¼mleri Ã§ekiliyor.", flush=True)

                # tam url
                target_url = f"{BASE_URL}{series_href}"

                # sayfayÄ± headless tarayÄ±cÄ± ile aÃ§ ve bekle
                page_html, ok = load_page_and_wait(driver, target_url, max_wait=PAGE_LOAD_MAX_WAIT)

                # Debug: ilk kÄ±smÄ± yazdÄ±r
                print(f"\nğŸ“„ {target_url} HTML (ilk {HTML_PRINT_LIMIT} karakter):")
                if page_html:
                    print(page_html[:HTML_PRINT_LIMIT])
                else:
                    print("âš ï¸ HTML boÅŸ geldi.")

                if not ok:
                    print(f"âš ï¸ {group}: Sayfa yÃ¼klenemedi veya Cloudflare challenge kaldÄ± (timeout).")
                    continue

                # Pars et
                try:
                    data = scrape_series_episodes_from_html(page_html)
                except Exception as e:
                    print(f"âš ï¸ {group} parsing hatasÄ±: {e}")
                    continue

                imdb_score = data.get("imdb", "-")
                episodes = data.get("episodes", [])

                if not episodes:
                    print(f"âš ï¸ {group}: HiÃ§ bÃ¶lÃ¼m bulunamadÄ±.")
                    continue

                for ep in episodes:
                    ep_href = ep.get("href")
                    ep_title_full = (ep.get("title") or "").upper()

                    season_match = re.search(r"(\d+)\.\s*SEZON", ep_title_full)
                    episode_match = re.search(r"(\d+)\.\s*BÃ–LÃœM", ep_title_full)
                    season = season_match.group(1).zfill(2) if season_match else "01"
                    episode = episode_match.group(1).zfill(2) if episode_match else "01"

                    tvg_name = f"{group.upper()} S{season}E{episode}"
                    extinf_line = (
                        f'#EXTINF:-1 tvg-id="" tvg-name="{tvg_name.upper()}" '
                        f'tvg-logo="{tvg_logo}" group-title="{group.upper()}", '
                        f'{group.upper()} {season}. SEZON {episode} (IMDb: {imdb_score} | YERLÄ° DÄ°ZÄ° | DIZIPAL)'
                    )

                    proxy_url = f"https://zerodayip.com/proxy/dizipal?url={BASE_URL}{ep_href}"

                    print(extinf_line, file=m3u_file, flush=True)
                    print(proxy_url, file=m3u_file, flush=True)

    finally:
        if driver:
            driver.quit()

    print(f"\nâœ… Dizipal m3u dosyasÄ± hazÄ±rlandÄ±: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()