# -*- coding: utf-8 -*-
# dizipal_episodes_selenium_m3u.py

"""
Gereksinimler:
Bu script'i Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce, aÅŸaÄŸÄ±daki kÃ¼tÃ¼phanelerin yÃ¼klÃ¼ olduÄŸundan emin olun:
pip install requests beautifulsoup4 selenium webdriver-manager
"""
import time
import json
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, WebDriverException
import sys

# -------------------------
# Ayarlar
# -------------------------
# Domain URL'si, gÃ¼ncel dizipal adresini iÃ§eren TXT dosyasÄ±nÄ± Ã§eker.
DOMAIN_URL = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizipal.txt"
# Dizilerin listelendiÄŸi JSON dosyasÄ± (Gerekli: dizipal/diziler.json)
JSON_FILE = "dizipal/diziler.json"
# Ã‡Ä±ktÄ± M3U dosyasÄ±nÄ±n adÄ±.
OUTPUT_FILE = "dizipalyerlidizi.m3u"

# TarayÄ±cÄ± ve bekleme ayarlarÄ±
INITIAL_PAGE_WAIT = 30       # Ana sayfa yÃ¼klendikten sonra sabit bekleme (saniye)
PAGE_LOAD_MAX_WAIT = 30      # Her bir dizi sayfasÄ± iÃ§in maksimum bekleme (saniye)
HTML_PRINT_LIMIT = 2000      # Debug iÃ§in terminale yazdÄ±rÄ±lacak HTML uzunluÄŸu (karakter)
RETRY_COUNT = 1              # Sayfa aÃ§ma denemesi (ÅŸu an sadece bir kere deneniyor)

# -------------------------
# Selenium baÅŸlatma (Chrome)
# -------------------------
def make_chrome_driver(headless=True):
    """Headless Chrome tarayÄ±cÄ±yÄ± baÅŸlatÄ±r."""
    options = webdriver.ChromeOptions()
    # Headless modunu ayarlama
    if headless:
        options.add_argument("--headless=new") # Yeni headless modu
    
    # Otomasyon ortamlarÄ± iÃ§in gerekli argÃ¼manlar
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--window-size=1920,1080")
    
    # TarayÄ±cÄ±yÄ± gizleme (Anti-bot algÄ±lamalarÄ±nÄ± azaltma)
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    try:
        # WebDriver'Ä± ChromeDriverManager ile otomatik indirme ve kurma
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # Navigator.webdriver Ã¶zelliÄŸini gizleme (CDP injection)
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                """
            },
        )
        return driver
    except WebDriverException as e:
        print(f"âŒ Chrome driver baÅŸlatÄ±lamadÄ±: {e}", file=sys.stderr)
        print("LÃ¼tfen Chrome'un kurulu ve sÃ¼rÃ¼mÃ¼nÃ¼n `webdriver-manager` tarafÄ±ndan desteklendiÄŸinden emin olun.", file=sys.stderr)
        raise

# -------------------------
# Sayfa yÃ¼kleme / bekleme
# -------------------------
def load_page_and_wait(driver, url, max_wait=PAGE_LOAD_MAX_WAIT, check_strings=None):
    """
    Belirtilen URL'yi yÃ¼kler ve iÃ§erik yÃ¼klenene veya zaman aÅŸÄ±mÄ± olana kadar bekler.
    """
    print(f"  -> URL yÃ¼kleniyor: {url}")
    driver.get(url)
    start = time.time()
    
    while True:
        page_source = driver.page_source
        title = driver.title or ""
        elapsed = time.time() - start

        # BaÅŸarÄ± KontrolÃ¼ 1: Ã–zel kontrol dizileri
        if check_strings:
            for s in check_strings:
                if s.lower() in page_source.lower():
                    return page_source, True
        
        # BaÅŸarÄ± KontrolÃ¼ 2: Cloudflare (CF) kontrolÃ¼ geÃ§ti mi?
        is_cf_challenge = "just a moment" in title.lower() or "enable javascript" in page_source.lower()
        is_cf_checking = "checking your browser" in page_source.lower()

        if not is_cf_challenge and not is_cf_checking:
            # CF kalktÄ±ysa, beklenen bir iÃ§erik var mÄ± kontrol et
            if "episode-item" in page_source or re.search(r"IMDB\s*Puan", page_source, re.I):
                print("  -> BaÅŸarÄ±lÄ± yÃ¼kleme (beklenen iÃ§erik bulundu).")
                return page_source, True
            # CF yazÄ±sÄ± yoksa ve sayfa deÄŸiÅŸmiÅŸse de kabul et (BazÄ± siteler sadece boÅŸ dÃ¶nebilir)
            if elapsed > 5: # 5 saniye geÃ§tikten sonra hala CF yoksa kabul et
                return page_source, True
        
        if elapsed >= max_wait:
            print(f"  -> Zaman aÅŸÄ±mÄ± ({max_wait}s).")
            return page_source, False
            
        time.sleep(1)

# -------------------------
# Scrape fonksiyonu (BeautifulSoup ile pars)
# -------------------------
def scrape_series_episodes_from_html(html_text):
    """HTML iÃ§eriÄŸinden IMDB puanÄ±nÄ± ve bÃ¶lÃ¼m linklerini ayrÄ±ÅŸtÄ±rÄ±r."""
    soup = BeautifulSoup(html_text, "html.parser")
    
    # 1. IMDB PuanÄ±
    imdb_score = "-"
    # IMDB PuanÄ±nÄ± iÃ§eren div'i bulma
    imdb_div = soup.find("div", class_="key", string=re.compile(r"IMDB", re.I))
    if imdb_div:
        value_div = imdb_div.find_next_sibling("div", class_="value")
        imdb_score = value_div.get_text(strip=True) if value_div else "-"
    
    # 2. BÃ¶lÃ¼m Listesi
    episodes = []
    # Standart bÃ¶lÃ¼m seÃ§icisini kullanma
    for ep_div in soup.select("div.episode-item a[href]"):
        ep_href = ep_div.get("href")
        ep_title_div = ep_div.select_one("div.episode")
        ep_title = ep_title_div.get_text(strip=True) if ep_title_div else "-"
        # Tam URL deÄŸil, sadece path ekle (BASE_URL ile birleÅŸtirilecek)
        episodes.append({"href": ep_href, "title": ep_title})
    
    # Fallback: EÄŸer standart seÃ§ici iÅŸe yaramazsa, tÃ¼m linkleri tarayÄ±p anahtar kelimeye gÃ¶re al
    if not episodes:
        for a in soup.select("a[href]"):
            href = a.get("href")
            txt = a.get_text(strip=True) or ""
            # Linkte /bolum veya metinde BÃ¶lÃ¼m geÃ§enleri al
            if href and ("/bolum" in href.lower() or "bÃ¶lÃ¼m" in txt.lower()):
                # Yine sadece path'i al
                episodes.append({"href": href, "title": txt or href})

    return {"imdb": imdb_score, "episodes": episodes}

# -------------------------
# Ana akÄ±ÅŸ
# -------------------------
def main():
    BASE_URL = None
    driver = None
    
    try:
        # 0) Chrome driver oluÅŸtur
        # Cloudflare zorluyorsa headless=False dene.
        headless = True
        driver = make_chrome_driver(headless=headless)

        # 1) DOMAIN_URL'den BASE_URL al
        print("ğŸŒ Domain bilgisi alÄ±nÄ±yor (tarayÄ±cÄ± ile)...", flush=True)
        # Direkt requests yerine selenium kullanÄ±yoruz (CF korumalÄ± olabilir)
        domain_page, ok = load_page_and_wait(driver, DOMAIN_URL, max_wait=10) # 10 saniye bekleme yeterli

        if not ok:
             # EÄŸer yÃ¼klenemediyse basit bir requests denemesi yapabiliriz (GitHub raw iÃ§in)
             import requests
             try:
                 response = requests.get(DOMAIN_URL, timeout=10)
                 if response.status_code == 200:
                     domain_page = response.text
                 else:
                     print(f"âŒ Domain URL'si yÃ¼klenemedi. HTTP Durumu: {response.status_code}", file=sys.stderr)
                     return
             except requests.RequestException as e:
                 print(f"âŒ Domain URL'si (requests) yÃ¼klenemedi: {e}", file=sys.stderr)
                 return

        # raw iÃ§eriÄŸi body iÃ§inde dÃ¼z metin olarak gelecektir
        m = re.search(r"https?://[^\s'\"<>]+", domain_page)
        if m:
            BASE_URL = m.group(0).strip()
        else:
            # fallback: sadece text olarak al
            text_only = re.sub(r"<[^>]+>", "", domain_page)
            # Ä°lk satÄ±rÄ± al ve temizle
            BASE_URL = text_only.strip().splitlines()[0].strip() if text_only.strip() else None

        if not BASE_URL:
            print("âŒ GeÃ§erli bir BASE_URL bulunamadÄ±.", file=sys.stderr)
            return
            
        print(f"ğŸŒ KullanÄ±lan BASE_URL: {BASE_URL}")

        # 2) Ana sayfa iÃ§in sabit bekleme (Site korumalarÄ± iÃ§in bazen gerekli)
        print(f"â³ Site korumalarÄ± iÃ§in {INITIAL_PAGE_WAIT} saniye bekleniyor...")
        time.sleep(INITIAL_PAGE_WAIT)

        # 3) JSON oku
        print(f"ğŸ“š {JSON_FILE} okunuyor.")
        try:
            with open(JSON_FILE, "r", encoding="utf-8") as f:
                series_data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ JSON dosyasÄ± bulunamadÄ±: {JSON_FILE}", file=sys.stderr)
            print("LÃ¼tfen 'dizipal/diziler.json' dosyasÄ±nÄ±n doÄŸru yolda olduÄŸundan emin olun.", file=sys.stderr)
            return
        except json.JSONDecodeError as e:
            print(f"âŒ JSON okuma hatasÄ±: {e}", file=sys.stderr)
            return

        # 4) M3U dosyasÄ± aÃ§
        print(f"ğŸ“ {OUTPUT_FILE} dosyasÄ±na yazÄ±lÄ±yor.")
        with open(OUTPUT_FILE, "w", encoding="utf-8") as m3u_file:
            print("#EXTM3U", file=m3u_file, flush=True)

            for series_href, info in series_data.items():
                group = info.get("group", "BÄ°LÄ°NMÄ°YOR")
                tvg_logo = info.get("tvg-logo", "")
                print(f"\nğŸ¬ {group} bÃ¶lÃ¼mleri Ã§ekiliyor...", flush=True)

                # Tam URL oluÅŸtur
                target_url = f"{BASE_URL}{series_href}"

                # SayfayÄ± headless tarayÄ±cÄ± ile aÃ§ ve bekle
                page_html, ok = load_page_and_wait(driver, target_url, max_wait=PAGE_LOAD_MAX_WAIT)

                # Debug: ilk kÄ±smÄ± yazdÄ±r
                print(f"  ğŸ“„ HTML (ilk {HTML_PRINT_LIMIT} karakter):")
                if page_html:
                    print(page_html[:HTML_PRINT_LIMIT].replace('\n', ' '))
                else:
                    print("  âš ï¸ HTML boÅŸ geldi.")

                if not ok:
                    print(f"  âš ï¸ {group}: Sayfa yÃ¼klenemedi veya Cloudflare challenge geÃ§ilemedi (timeout).")
                    continue

                # Pars et
                try:
                    data = scrape_series_episodes_from_html(page_html)
                except Exception as e:
                    print(f"  âš ï¸ {group} parsing hatasÄ±: {e}", file=sys.stderr)
                    continue

                imdb_score = data.get("imdb", "-")
                episodes = data.get("episodes", [])

                if not episodes:
                    print(f"  âš ï¸ {group}: HiÃ§ bÃ¶lÃ¼m bulunamadÄ±. Sonraki diziye geÃ§iliyor.")
                    continue

                for ep in episodes:
                    ep_href = ep.get("href")
                    ep_title_full = (ep.get("title") or "").upper()

                    # Sezon ve BÃ¶lÃ¼m numarasÄ±nÄ± Ã§ekme (Ã–r: 1. SEZON 2. BÃ–LÃœM)
                    season_match = re.search(r"(\d+)\.\s*SEZON", ep_title_full)
                    episode_match = re.search(r"(\d+)\.\s*BÃ–LÃœM", ep_title_full)
                    season = season_match.group(1).zfill(2) if season_match else "01"
                    episode = episode_match.group(1).zfill(2) if episode_match else "01"

                    # M3U Gerekli Veriler
                    tvg_name = f"{group.upper()} S{season}E{episode}"
                    
                    # EXTINF SatÄ±rÄ±
                    extinf_line = (
                        f'#EXTINF:-1 tvg-id="" tvg-name="{tvg_name}" '
                        f'tvg-logo="{tvg_logo}" group-title="{group.upper()}",'
                        f'{group.upper()} {int(season)}. SEZON {int(episode)}. BÃ–LÃœM (IMDb: {imdb_score} | YERLÄ° DÄ°ZÄ° | DIZIPAL)'
                    )
                    
                    # Proxy URL
                    # ep_href'in BASE_URL'siz sadece path olmasÄ± gerekiyor.
                    proxy_url = f"https://zerodayip.com/proxy/dizipal?url={BASE_URL}{ep_href}"

                    print(extinf_line, file=m3u_file, flush=True)
                    print(proxy_url, file=m3u_file, flush=True)

    except Exception as e:
        print(f"\nâŒ Kritik Hata: {e}", file=sys.stderr)
        
    finally:
        if driver:
            print("\nğŸ§¹ TarayÄ±cÄ± kapatÄ±lÄ±yor.")
            driver.quit()

    if BASE_URL:
        print(f"\nâœ… Dizipal m3u dosyasÄ± hazÄ±rlandÄ±: {OUTPUT_FILE}")
    else:
        print("\nâŒ Ä°ÅŸlem tamamlanamadÄ±. LÃ¼tfen hata mesajlarÄ±nÄ± kontrol edin.", file=sys.stderr)

if __name__ == "__main__":
    main()