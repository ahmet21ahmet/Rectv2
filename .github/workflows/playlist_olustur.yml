# GitHub Actions workflow to generate and update an M3U playlist
# This workflow is designed to scrape the HDFilmCehennemi website,
# mirroring the logic from the provided Kotlin source file.

name: Generate and Update M3U Playlist

on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
  # Runs the job automatically at midnight every day
  schedule:
    - cron: '0 0 * * *'

env:
  # The main URL of the website to be scraped, taken from the Kotlin file.
  MAIN_URL: "https://www.hdfilmcehennemi.life"

jobs:
  generate-m3u:
    runs-on: ubuntu-latest
    permissions:
      # Required to allow the workflow to commit the updated playlist to the repository
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Install requests for HTTP calls and BeautifulSoup for HTML parsing
        pip install requests beautifulsoup4 tqdm

    - name: Generate M3U playlist
      run: |
        # Create the Python script that will perform the scraping and generation
        cat << 'EOF' > generate_playlist.py
        import requests
        import re
        import os
        import datetime
        from bs4 import BeautifulSoup
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from tqdm import tqdm

        # --- Configuration ---
        MAIN_URL = os.environ.get('MAIN_URL').strip('/')
        # Set a reasonable limit for pages to scrape per category to prevent infinite loops
        MAX_PAGES_PER_CATEGORY = 20
        # Number of parallel workers for fetching stream details
        MAX_WORKERS = 10
        HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Referer": f"{MAIN_URL}/"
        }

        # --- Helper Functions ---

        def get_soup(url):
            """Fetches a URL and returns a BeautifulSoup object."""
            try:
                response = requests.get(url, headers=HEADERS, timeout=20)
                response.raise_for_status()
                return BeautifulSoup(response.content, 'html.parser')
            except requests.exceptions.RequestException as e:
                print(f"Error fetching {url}: {e}")
                return None

        def get_stream_url_from_item(item_url):
            """
            Visits a movie/episode page, finds the 'film_id', and fetches the stream URL.
            This mimics the 'load' function in the Kotlin source.
            """
            try:
                # 1. Get the item page content
                page_content = requests.get(item_url, headers=HEADERS, timeout=20).text

                # 2. Find the film_id using regex, as in the Kotlin file
                film_id_match = re.search(r'var film_id\s*=\s*"(\d+)";', page_content)
                if not film_id_match:
                    return None

                film_id = film_id_match.group(1)

                # 3. Make the POST request to the player ajax endpoint
                ajax_url = f"{MAIN_URL}/player-ajax.php"
                payload = {
                    'action': 'get_player_sources',
                    'film_id': film_id
                }
                ajax_response = requests.post(ajax_url, data=payload, headers=HEADERS, timeout=20)
                ajax_response.raise_for_status()
                sources = ajax_response.json().get('data', [])

                # 4. Find the first m3u8 source URL
                for source in sources:
                    if source.get('file', '').endswith('.m3u8'):
                        return source['file']

            except Exception as e:
                # print(f"Error getting stream for {item_url}: {e}")
                pass
            return None

        # --- Main Processing Functions ---

        def process_category(category_path, group_title):
            """
            Scrapes a generic category (like movies or hdtv) page by page.
            """
            print(f"\nProcessing Category: {group_title}")
            all_items = []
            for page_num in range(1, MAX_PAGES_PER_CATEGORY + 1):
                page_url = f"{MAIN_URL}{category_path}page/{page_num}/"
                soup = get_soup(page_url)
                if not soup:
                    break

                items_on_page = soup.select("div.ml-item")
                if not items_on_page:
                    print(f"No items found on page {page_num}. Concluding '{group_title}'.")
                    break

                print(f"Found {len(items_on_page)} items on page {page_num} for '{group_title}'")
                for item in items_on_page:
                    link_tag = item.find("a", href=True)
                    img_tag = item.find("img", src=True)
                    title = link_tag.get('title', 'Unknown Title')
                    url = link_tag['href']
                    poster = img_tag['src'] if img_tag else ''
                    all_items.append({'title': title, 'url': url, 'poster': poster, 'group': group_title})
            
            return all_items

        def process_series():
            """
            Handles the specific multi-level scraping required for TV series.
            """
            print("\nProcessing Category: Diziler")
            series_list = process_category("/category/dizi-izle/", "Diziler")
            all_episodes = []

            # Scrape each series page to find its episodes
            for series in tqdm(series_list, desc="Fetching episodes for each series"):
                series_soup = get_soup(series['url'])
                if not series_soup:
                    continue

                # Find all episode links on the series page
                episode_tags = series_soup.select("ul.episodios li a")
                for ep_tag in episode_tags:
                    ep_title = f"{series['title']} - {ep_tag.text.strip()}"
                    ep_url = ep_tag['href']
                    all_episodes.append({
                        'title': ep_title,
                        'url': ep_url,
                        'poster': series['poster'],
                        'group': series['group']
                    })
            return all_episodes

        def fetch_and_write_streams(items, file_handle):
            """
            Uses a thread pool to fetch stream URLs concurrently and write them to the M3U file.
            """
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_item = {executor.submit(get_stream_url_from_item, item['url']): item for item in items}

                for future in tqdm(as_completed(future_to_item), total=len(items), desc=f"Fetching streams for {items[0]['group']}"):
                    item = future_to_item[future]
                    stream_url = future.result()

                    if stream_url:
                        f.write(f'#EXTINF:-1 tvg-logo="{item["poster"]}" group-title="{item["group"]}",{item["title"]}\n')
                        f.write(f"{stream_url}\n\n")


        # --- Main Execution ---

        if __name__ == "__main__":
            print("Starting M3U playlist generation...")

            # Scrape all categories first to gather all item URLs
            movies = process_category("/category/film-izle/", "Filmler")
            hdtv = process_category("/category/hdtv/", "HDTV")
            series_episodes = process_series()

            all_content = movies + hdtv + series_episodes

            print(f"\nTotal Movies found: {len(movies)}")
            print(f"Total TV Channels found: {len(hdtv)}")
            print(f"Total Series Episodes found: {len(series_episodes)}")
            print(f"Total items to process: {len(all_content)}")

            with open("playlist.m3u", "w", encoding="utf-8") as f:
                f.write("#EXTM3U\n")
                f.write(f"# Generated at: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\n")
                f.write(f"# Source: {MAIN_URL}\n\n")

                # Process and write streams for each category
                if movies:
                    fetch_and_write_streams(movies, f)
                if hdtv:
                    fetch_and_write_streams(hdtv, f)
                if series_episodes:
                    fetch_and_write_streams(series_episodes, f)

            print("\nPlaylist generation completed successfully!")
        EOF

        # Execute the generated Python script
        python generate_playlist.py

    - name: Commit and push changes
      # This step runs only if the previous steps were successful
      if: success()
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add playlist.m3u
        # The following command commits only if there are actual changes to the playlist file
        git diff --quiet && git diff --staged --quiet || git commit -m "Update M3U playlist [auto]"
        git push